# vLLM Server Configuration
# This file can be used to configure the vLLM server with all available options

# Required: The model to load (e.g., HuggingFace model ID or local path)
model: "unsloth/Llama-3.2-1B-Instruct"

# Optional: Model revision (e.g., commit hash or branch name)
revision: null

# Optional: Number of GPUs to use for tensor parallelism
tensor_parallel_size: 1

# Optional: Host to bind the server to
host: "0.0.0.0"

# Optional: Port to bind the server to
port: 8000

# Optional: GPU memory utilization (0.0 to 1.0)
gpu_memory_utilization: 0.9

# Optional: Data type for model weights (auto, float16, bfloat16, float32)
dtype: "auto"

# Optional: Maximum sequence length for the model
max_model_len: 1024

# Optional: Whether to enable prefix caching
enable_prefix_caching: false 