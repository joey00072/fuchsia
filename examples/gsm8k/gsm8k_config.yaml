# Shared configuration (used by both trainer and server)
shared:
  rollout:
    group_size: &group_size 8
  sampling:
    max_tokens: &max_tokens 2048
    repetition_penalty: &repetition_penalty 1.0
    temperature: &temperature 0.8
    top_k: &top_k -1
    top_p: &top_p 1.0
    min_p: &min_p 0.0
    logprobs: &logprobs 1
  runtime:
    single_gpu: true
    lora_path: "./lora_weights"
  transfer:
    mode: &transfer_mode "api"
    queue_dir: &transfer_queue_dir "/tmp/fuchsia_sample_queue"
    poll_interval: &transfer_poll_interval 0.25
    clear_on_start: &transfer_clear_on_start false

# Generation configuration (trainer-facing compatibility)
generation: &generation
  max_len: *max_tokens
  group_size: *group_size
  temperature: *temperature
  top_k: *top_k
  top_p: *top_p
  min_p: *min_p
  batch_size: &generation_batch_size 4

# Model configuration
model:
  name: "joey00072/Llama-3.2-1B-Instruct-cold-start-ft2"
  revision: null
  dtype: "bfloat16"
  max_model_len: *max_tokens

# LoRA configuration
lora:
  enabled: true
  r: 16
  alpha: 16
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"]

# Trainer configuration
trainer:
  group_size: *group_size
  batch_size: *generation_batch_size
  grad_accumulation_steps: *generation_batch_size
  lr: 0.00001
  weight_decay: 0.1
  beta: 0.01
  epsilon: 0.2
  epsilon_high: 0.28
  log_wandb: true
  wandb_project: "fuchsia-dgrpo"
  num_policy_updates: 1
  loss_type: "cispo"
  gradient_checkpointing:
    enabled: true
    cpu_offloading: true

# Server configuration
server:
  host : "0.0.0.0"
  port: 8000
  gpu_memory_utilization: 0.8
  tensor_parallel_size: 1
  enable_prefix_caching: false
  buffer_size: *generation_batch_size
  generation_batch_size: *generation_batch_size
  transfer:
    mode: *transfer_mode
    queue_dir: *transfer_queue_dir
    poll_interval: *transfer_poll_interval
    clear_on_start: *transfer_clear_on_start
  quantization: 
  vllm:
    max_tokens: *max_tokens
    n: *group_size
    repetition_penalty: *repetition_penalty
    temperature: *temperature
    top_p: *top_p
    top_k: *top_k
    min_p: *min_p
    logprobs: *logprobs

# Dataset configuration
dataset:
  name: "gsm8k"
  split: "main"
  max_samples: null
  field: "text"

# Training configuration
training:
  max_epochs: 1
  max_iterations: 100000
  save_steps: 100
  eval_steps: 50
  output_dir: "gsm8k_output" 


# 8k 246.2914233800002
# 16k 537.8559579419998
